---
layout: post
title:  "A march in progress"
date:   2020-04-24 01:03:29 +0200
categories: progress
comments: true
---
A week of math and ~mayhem

<!--more-->

I picked two courses for this week. The first one, Deepbayes, a Bayesian ML 6 day study camp by HSE. Almost done with this, few hours of classes left. This certainly won't go as one my favorite series though I loved few talks. I didn't have any idea about fairness in ML. Novi Quadrianto gave an excellent talk on it highlighting all the main criterions used, how it affects accuracy, what to choose and why. Apart from this, I really liked the sessions on GANs due to its amazing progression in theoretical work in making it better. Another talk, on adaptive skipgram was great as well. Bayesian deep learning is picking up in the past few years, would love to keep an eye out on it, see how it progresses. 

Now, the second lecture series I chose was Deep unsupervised learning 2020, UC Berkeley. Super fun long lectures, couple of videos in, I'm loving it. This has a different flow to it and I really like it. Now, there is a bit of overlap between the 2 courses though I feel, this focuses more on the architecture side, brainstorming ideas on it and exploring better representations. Meanwhile Beepbayes was mostly on math involved in practical optimisation of different inference methods. 

Now the plan is to complete both the courses by this weekend, then coding on the latter course for the rest of the week. This area also has a large number of recent good papers to explore so I can try those out as well. In this space, seems like the problem mainly is better training methods(which can be made using better input representation for learning), latent space utilisation for pretraining for other tasks etc. Would love to read more on these problem statements. 

On the paper side, put in a lot of hours in cleaning autoencoder code to evaluate generated images properly. Now that it is done, training for new defect SNR can be done automatically on the background for hours, look at images later and pick the model needed. Along with that, once the draft changes are implemented in the next day or two, I'll be done with that for a couple of days. Then I can finish the other LSTM paper draft. 

On Kaggle, found a piece of code which does cross validation optimally using pytorch without torchtext, now I can structure it as a dataset and get a good classification baseline code. Once that is done, I can add more models to it!!

Didn't really get a lot of time to solve leetcode problems. Just a couple of mediums, reading through DP solutions of hard problems. 

Now, its close to 12:30AM Saturday, have to check out this Uber AI reading group going live in few minutes, so adios until next week!
